\documentclass[oneside, abstracton, titlepage]{scrartcl}

\usepackage[left=2.5cm,right=2.6cm,top=3cm,bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[intlimits]{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{xcolor}
\usepackage{subcaption}

\usepackage{graphicx}

\newsavebox{\selvestebox}
\newenvironment{colbox}[1]
{\newcommand\colboxcolor{#1}%
	\begin{lrbox}{\selvestebox}%
		\begin{minipage}{\dimexpr\columnwidth-2\fboxsep\relax}}
		{\end{minipage}\end{lrbox}%
	\begin{center}
		\colorbox[HTML]{\colboxcolor}{\usebox{\selvestebox}}
\end{center}}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
		\left.\kern-\nulldelimiterspace % automatically resize the bar with \right
		#1 % the function
		\vphantom{\big|} % pretend it's a little taller at normal size
		\right|_{#2} % this is the delimiter
}}

\begin{document}
	\title{Discovering governing reactions from concentration data}
	\maketitle

	\section{Introduction}
	\textcolor{red}{Cite and mention previous works.}
		
	When presented with a time series of possibly noisy non-equilibrium concentration fluctuations of some species as output of, e.g., measurements from experiments or simulations that were parameterized by microscopic rates, one can ask for the corresponding macroscopic rates and a generating reaction network.
	In this paper we present an application of the shallow learning method SINDy \cite{Brunton2015}. By sparse regression, it is able to identify generating nonlinear dynamics in data that stems from dynamical systems. The parsimonious nature of the results avoids overfitting and provides interpretability.
	In our application we, as opposed to the SINDy method, estimate parameters which are coupled across the equations of the arising ODE system, i.e., we look for specific reactions and their rate constants that might have lead to the observations instead of net flux across species.
	We demonstrate the algorithm on a biologically motivated reaction kinetics system in three different scenarios of measurement: When there is no noise in the data we can find all relevant processes of the ground truth. If there is noise in the data we converge to the correct reaction network and rates with decreasing noise. The last scenario deals with the case that there are two realizations with different initial conditions, for which one can also show convergence to the correct model with decreasing levels of noise.

	\section{The method}
	The underlying model are reaction-rate equations subject to the law of mass action. To this end, let $S$ be the number of species, then the observed concentration at a time $t$ can be represented by a vector
	\begin{align}
	\mathbf{x}(t)=\begin{pmatrix}
	x_1(t)\\ \vdots \\ x_S(t)
	\end{pmatrix}\in \mathbb{R}^S.
	\end{align}
	Further, one can choose $R$ possible ansatz reactions with their respective reaction functions
	\begin{align}
	\textbf{y}_r(\textbf{x}(t))=\begin{pmatrix}
	y_{r,1}(\textbf{x}(t)) \\ \vdots \\ y_{r,S}(\textbf{x}(t))
	\end{pmatrix},\quad r=1,\ldots,R,
	\end{align}
	so that the change of concentration for species $i$ at time $t$ is represented by the dynamical system
	\begin{align}
	\dot{\textbf{x}}_i(t) = \sum_{r=1}^{R}y_{r,i}(\textbf{x}(t))\xi_r,\quad i=1,\ldots, S,
	\label{method:the-system}\end{align}
	where $\xi_r$ are the to-be estimated macroscopic rate constants.

	When presented with a time series consisting of $T$ observations at points in time $t_1<\ldots < t_T$, the data can be represented as a matrix
	\begin{align}
	\textbf{X} = \begin{pmatrix}
		x_1(t_1) & x_2(t_1) & \cdots & x_S(t_1) \\
		x_1(t_2) & x_2(t_2) & \cdots & x_S(t_2) \\
		\vdots   & \vdots   & \ddots & \vdots   \\
		x_1(t_T) & x_2(t_T) & \cdots & x_S(t_T)
	\end{pmatrix} \in \mathbb{R}^{T\times S}.
	\end{align}
	Given this matrix, a library $\Theta: \mathbb{R}^{T\times S}\to \mathbb{R}^{T \times S\times R},\;\mathbf{A}\mapsto \begin{pmatrix} \theta_1(\mathbf{A}) & \theta_2(\mathbf{A}) & \cdots & \theta_R(\mathbf{A}) \end{pmatrix}$ of $R$ ansatz reactions can be proposed with corresponding reaction functions
	\begin{align}
		\theta_r(\mathbf{A}) = \begin{pmatrix}
		\textbf{y}_r(\mathbf{A}_{1*})^T \\ \vdots \\ \textbf{y}_r(\mathbf{A}_{T*})^T
		\end{pmatrix}\in \mathbb{R}^{T\times S},\quad r=1,\ldots,R,
	\label{method:the-reactions}\end{align}
	where $\textbf{A}_{i*}$ denotes the $i$-th row in $\textbf{A}$. Applying the concentration trajectory to the library yields $\Theta(\textbf{X})\in\mathbb{R}^{T\times S\times R}$. Following the approach of SINDy, the goal is to find coefficients $\Xi = \begin{pmatrix} \xi_1 & \xi_2 & \cdots & \xi_R
	\end{pmatrix}^T$, so that
	\begin{align}
	\dot{\textbf{X}} = \Theta(\textbf{X})\Xi = \sum_{r=1}^{R}\theta_r(\textbf{X})\xi_r.
	\end{align}
	In particular, the system is linear in the coefficients $\Xi$, which makes regression tools such as elastic net regularization \cite{Zou2005} applicable. To this end, one can consider the minimization problem to find $\hat{\Xi}$ such that
	\begin{align}
		\hat{\Xi} = \underset{\Xi}{\arg\min}\left( \frac{1}{2T}\left\| \dot{\textbf{X}} - \Theta(\textbf{X})\Xi \right\|_F^2 + \alpha\lambda\|\Xi\|_1 + \alpha(1-\lambda)\|\Xi\|_2^2 \right) \quad \text{subject to }\Xi \geq 0,
	\label{method:minimizationproblem}\end{align}
	where $\|\cdot\|_F$ denotes the Frobenius norm, $\lambda\in[0,1]$ a hyperparameter that interpolates linearly between LASSO \cite{Tibshirani1996, Hastie2009} and Ridge \cite{Hoerl1} methods, and $\alpha\geq 0$ is a hyperparameter that, depending on $\lambda$, can induce sparsity and give preference to smaller solutions in the $L_1$ or $L_2$ sense.

	For $\alpha=0$ the minimization problem reduces to constrained least-squares. In order to solve (\ref{method:minimizationproblem}) the numerical sequential least-squares minimizer SLSQP \cite{Kraft1988} is applied via the software package SciPy \cite{SciPy}. Since only the concentration data $\mathbf{X}$ is available but not its temporal derivative $\dot{\mathbf{X}}$, it is approximated numerically by second order finite differences with the exception of boundary data. Once the pair $(\mathbf{X}, \dot{\mathbf{X}})$ is obtained, the problem becomes invariant under temporal reordering. Hence, when presented with multiple trajectories the data matrices $\mathbf{X}_i$ and $\dot{\mathbf{X}}_i$ can simply be concatenated.
	
    \section{Example system: Regulation of gene expression}\label{sec:generegulation}

    To demonstrate the method a gene-regulatory network is estimated from time series of molecule-concentrations.
    
    To this end, let $\mathrm{A}$, $\mathrm{B}$, and $\mathrm{C}$ be three species of proteins which are being translated from a corresponding $\mathrm{mRNA}$ molecule. Each $\mathrm{mRNA}$ in turn has a corresponding $\mathrm{DNA}$ which it is transcribed from. The proteins and $\mathrm{mRNA}$ molecules decay over time whereas the $\mathrm{DNA}$ concentration remains constant.
    
    The following basic reactions can be formulated
	\begin{align*}
		\mathrm{DNA}_i &\rightharpoonup \mathrm{DNA}_i + \mathrm{mRNA}_i &&\text{(transcription)},\\
		\mathrm{mRNA}_i &\rightharpoonup \mathrm{mRNA}_i + i &&\text{(translation)},\\
		\mathrm{mRNA}_i &\rightharpoonup \emptyset &&\text{(decay of mRNA)},\\
		i &\rightharpoonup \emptyset &&\text{(decay of protein)},
    \end{align*}
    for each of the species $i\in S:=\{\mathrm{A},\mathrm{B},\mathrm{C}\}$.

    The protein concentrations of $\mathrm{A}$, $\mathrm{B}$, and $\mathrm{C}$ regulate each other in a negative way by hindering the transcription process. In the law of mass action model we account for this by a reaction. A repression of species $j\in S$ by species $i\in S$ can be modeled as
    \begin{align*}
    i + \mathrm{mRNA}_j \rightharpoonup i.
    \end{align*}

    In our example proteins of type $\mathrm{A}$ regulate the $\mathrm{mRNA}_\mathrm{B}$ molecules, proteins of type $\mathrm{B}$ regulate the $\mathrm{mRNA}_\mathrm{C}$ molecules and proteins of type $\mathrm{C}$ regulate the $\mathrm{mRNA}_\mathrm{A}$ molecules. The network of all species and reactions is depicted in Fig.~\ref{fig:network}. This serves as reference model to generate the time series of concentrations.
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{./figures_tex/scheme.pdf}
        \caption{The regulation network example described in Sec.~\ref{sec:generegulation}. \textbf{(a)}: Each circle depicts a species, each arrow corresponds to one reaction. Blue arrows denote transcription from DNA to $\mathrm{mRNA}$, green arrows denote translation from $\mathrm{mRNA}$ to protein, and red arrows denote the regulatory network. \textbf{(b)}: Concentration timeseries generated from the reaction network shown in~(a). The initial condition prescribes positive concentration values only for $\mathrm{B}$ protein and $\mathrm{mRNA}_\mathrm{A}$ species. This initial condition is used in the subsequent sections for further analysis.}
        \label{fig:network}
    \end{figure}

	\section{Results} \label{sec:results}
	
	The proposed estimation method is designed to find reaction networks and rate constants from experimental data. In the following the method is applied to data generated from the example described in Section \ref{sec:generegulation}.

	In Sec.~\ref{sec:case-1} is assumed that the concentration data can be obtained without noise and it is shown that while the minimization problem (\ref{method:minimizationproblem}) without regularization, i.e., $\alpha=0$, yields rates that fit well to the input data, the original rates cannot be recovered while adding $L_1$ regularization and applying a cutoff recovers the underlying reaction network.
	
	In Sec.~\ref{sec:case-2} the example data was generated using the Gillespie SSA \textbf{[cite]}. While the concentrations now contain stochastic noise, it is assumed that the initial state can be reproduced exactly and multiple measurements with that exact initial condition can be performed. It is shown that in the limit of many measurements a good fit can reliably be obtained.
	
	The last example is described in Sec.~\ref{sec:case-3} and is an extension of the situation in Sec.~\ref{sec:case-2} to multiple initial conditions. Due to more varied data and more statistics a good fit can be obtained with fewer measurements than in the case of just one initial condition.
	
	%measurement scenarios are considered that differ in how many time series are available and the level of noise that the data is subject to:
	%\begin{enumerate}
	%	\item The initial state of the measurement apparatus can be reproduced exactly. There is only one set of generating reaction rates. Each frame of a time series is subject to noise. The outcome of this scenario is one time series of concentrations with a certain noise-level.
	%	\item The process is observed multiple times. The initial state of the measurement apparatus differs in each observation. The outcome of this scenario is multiple time series each with a certain noise level and different initial states.
	%\item The process is observed multiple times. The initial state of the measurement apparatus differs in each observation. Additionally the generating reaction rates differ slightly in each observation. The outcome of this scenario is multiple time series each with a given noise level, different initial states and slightly perturbed underlying reaction rates.
	%\end{enumerate}

	\subsection{The noiseless case}\label{sec:case-1}
	
	In this section noiseless data is generated from integrating the reaction-rate equation (\ref{method:the-system}) of the system described in Sec.~\ref{sec:generegulation}. The minimization problem (\ref{method:minimizationproblem}) is solved with respect to this data. When setting the hyperparameter $\alpha=0$ one obtains linear least-squares regression with a constraint on the non-negativity of the estimated reaction rates.
	
	When applying constrained linear least-squares regression, one can observe that the sparsity pattern does not match the generating reaction network and the reaction responsible for the decay of $\mathrm{A}$ particles is completely ignored (Fig.~\ref{fig:case-1-sparsity-pattern}). For a suitable choice of hyperparameters $\alpha \approx 1.91\cdot 10^{-7}$, $\lambda=1$, and a cutoff $\kappa = 0.22$ a sparse solution can be obtained and also the decay reaction can be recovered. The cutoff $\kappa$ is applied so that for all reactions with an estimated rate $\hat{\xi} < \kappa$ a rate of $\hat{\xi} = 0$ is assigned. A circle with size corresponding to a rate of $\kappa$ is depicted in Fig.~\ref{fig:case-1-sparsity-pattern} in green.
	
	\begin{figure}
		\centering
		\includegraphics[width=.9\textwidth]{./figures_tex/sparsity_pattern}
		\caption{Estimated reaction rates in the system described in Sec.~\ref{sec:case-1}. The y and x axes contain reaction educts and products, respectively. A circle at position $(i,j)$ represents a reaction $i\rightharpoonup j$ whose rate has a linear relation with the area of the circle. The black outlines denote the reactions with which the system was generated and contain the respective rate value. Red crosses denote reactions that were used as additional ansatz reactions. Blue circles are estimated by linear least-squares and orange circles depict rates which were obtained by solving the minimization problem (\ref{method:minimizationproblem}). The latter rates are subject to a cutoff $\kappa=0.22$ corresponding to the green circle's area under which a sparse solution with the correct processes can be recovered. If a certain rate was estimated in both cases, two wedges instead of one circle are displayed.}
		\label{fig:case-1-sparsity-pattern}
	\end{figure}

	\textbf{Todo}:
	\begin{enumerate}
		\item mention finite statistics
		\item how does LSQ compensate for $A\to\emptyset$
		\item ansatzfunctions used in $\Theta$
	\end{enumerate}
	
	\subsection{Data with stochastic noise}\label{sec:case-2}
	For Gillespie, the initial condition contains hundreds of particles such that the average over many realizations converges against the reaction-rate equation.
	
	\begin{itemize}
		\item Same setup as in Section \ref{sec:case-1}
		\item def. of failure rate
		\item $10$-fold cross validation discarding fold with test set at the (temporal) beginning (reason: more robust hyperparameter estimation as statisticial dependence of our data is higher at the initial equilibration phase, shuffle didn't really work well)
		\item CV was independently carried out 10 times to obtain avg/std
		\item comparison to LSQ: LSQ unreliable \textbf{[todo]}
		\item in the limit of small noise one gets a good fit
		\item obtain cutoff in noise free case -> compute failure rate (number of false positives and negatives)
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width=.5\columnwidth]{./figures_tex/case2}
		\caption{Convergence of the estimation error when estimating the system described in Sec.~\ref{sec:case-1} with decreasing levels of noise by application of the minimization problem (\ref{method:minimizationproblem}). The level of noise is regulated by the number of measurements being averaged. The hyperparameters for each level of noise were estimated using 10-fold cross validation. This was repeated 10 times giving rise to the mean and standard deviation depicted by dark blue graph and shaded blue area, respectively. The failure rate is the number of incorrectly identified processes. The estimation error is given by the $L_1$ difference of the generating reaction rates $\hat{\xi}$ and the estimated reaction rates $\xi_\mathrm{CV}$.}
		\label{fig:case2-convergence}
	\end{figure}

	\subsection{Multiple initial conditions}\label{sec:case-3}
	
	\begin{itemize}
		\item extend situation of Section \ref{sec:case-3} to multiple initial conditions (zipped / reorder)
		\item $20$-fold CV discarding 1st fold (see sec 2)
		\item compute failure rate
		\item model can reliably be recovered in the limit of small noise
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width=\columnwidth]{./figures_tex/case3}
		\caption{Convergence of the estimation error when using two concatenated concentration timeseries with different initial conditions. \textbf{(a)}: A realization of the concatenated timeseries. The first initial condition is identical to what was used in Sec.~\ref{sec:case-1} and Sec.~\ref{sec:case-2}. The second initial condition prescribes positive initial concentrations for $\mathrm{mRNA}_\mathrm{A}$, $\mathrm{B}$, and $\mathrm{C}$ species. \textbf{(b)}: Analogously to Fig.~\ref{fig:case2-convergence} with the difference that 20-fold cross validation was used for hyperparameter estimation.}
	\end{figure}
    
	\section{Conclusion}
	\begin{colbox}{F8E0E0}
		todo
	\end{colbox}
	In this work we have successfully applied and extended the SINDy method to not only parsimoniously detect potentially nonlinear terms in a dynamical system from noisy data, but also yield, in this case, a sparse set of rates with respect to generating reactions (\ref{method:the-reactions}).

	%In two examples it was demonstrated that despite noisy data and unavailable derivative measurements, a parsimonious generating reaction network that is qualitatively able to explain the observed data can be estimated.
	%In particular it was shown in the first example that if there is no ambiguity in the underlying model and ansatz reaction library, the actual rates can be recovered with decreasing time step, i.e., increasing resolution of the jump process.
	%In the second example we could obtain an even simpler model than what was used to generate data by making use of sparse regression and cross-validation.

	\newpage
% 	\bibliographystyle{alpha}
	\bibliographystyle{abbrv}
	\bibliography{bibliography.bib}
	\newpage	
	\section{Appendix}\label{sec:appendix}
	\begin{table}[h]
		\centering
		\scalebox{.75}{
			\begin{tabular}{rclcc}
				\multicolumn{3}{c}{Reaction} & rate & description \\ \hline\noalign{\vskip .1cm}
				$\mathrm{DNA}_\mathrm{A}$ &$\rightharpoonup$& $\mathrm{DNA}_\mathrm{A} + \mathrm{mRNA}_\mathrm{A}$ & $k_1 = 1.8$ & transcription of $\mathrm{mRNA}_\mathrm{A}$\\
				$\mathrm{mRNA}_\mathrm{A}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{A} + \mathrm{A}$ & $k_2 = 2.1$ & translation of $\mathrm{A}$ proteins\\
				$\mathrm{mRNA}_\mathrm{A}$ &$\rightharpoonup$& $\emptyset$ & $k_3 = 1.3$ & $\mathrm{mRNA}_\mathrm{A}$ decay\\
				$\mathrm{A}$ &$\rightharpoonup$& $\emptyset$ & $k_4 = 1.5$ & decay of $\mathrm{A}$ proteins\\
				$\mathrm{DNA}_\mathrm{B}$ &$\rightharpoonup$& $\mathrm{DNA}_\mathrm{B} + \mathrm{mRNA}_\mathrm{B}$ & $k_5 = 2.2$ & transcription of $\mathrm{mRNA}_\mathrm{B}$\\
				$\mathrm{mRNA}_\mathrm{B}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{B} + \mathrm{B}$ & $k_6 = 2.0$ & translation of $\mathrm{B}$ proteins\\
				$\mathrm{mRNA}_\mathrm{B}$ &$\rightharpoonup$& $\emptyset$ & $k_7 = 2.0$ & $\mathrm{mRNA}_\mathrm{B}$ decay\\
				$\mathrm{B}$ &$\rightharpoonup$& $\emptyset$ & $k_8 = 2.5$ & decay of $\mathrm{B}$ proteins\\
				$\mathrm{DNA}_\mathrm{C}$ &$\rightharpoonup$& $\mathrm{DNA}_\mathrm{C} + \mathrm{mRNA}_\mathrm{C}$ & $k_9 = 3.2$ & transcription of $\mathrm{mRNA}_\mathrm{C}$\\
				$\mathrm{mRNA}_\mathrm{C}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{C} + \mathrm{C}$ & $k_{10} = 3.0$ & translation of $\mathrm{C}$ proteins\\
				$\mathrm{mRNA}_\mathrm{C}$ &$\rightharpoonup$& $\emptyset$ & $k_{11} = 2.3$ & $\mathrm{mRNA}_\mathrm{C}$ decay\\
				$\mathrm{C}$ &$\rightharpoonup$& $\emptyset$ & $k_{12} = 2.5$ & decay of $\mathrm{C}$ proteins\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{13} = 0$ & self regulation of $\mathrm{A}$ proteins\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{14} = 0$ & self regulation of $\mathrm{B}$ proteins\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{15} = 0$ & self regulation of $\mathrm{C}$ proteins\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{16} = 0$ & cyclic regulation of $\mathrm{A}$ proteins\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{17} = 0$ & cyclic regulation of $\mathrm{B}$ proteins\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{18} = 0$ & cyclic regulation of $\mathrm{C}$ proteins\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{16} = 6.0$ & cyclic regulation of $\mathrm{A}$ proteins\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{17} = 4.0$ & cyclic regulation of $\mathrm{C}$ proteins\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{18} = 3.0$ & cyclic regulation of $\mathrm{B}$ proteins\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{A}$ & $k_{19} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{B}$ & $k_{20} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{A}$ & $k_{21} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{B}$ & $k_{22} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{C}$ & $k_{23} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{A} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{A}$ & $k_{24} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{B}$ & $k_{25} = 0$ & artificial fusion\\
				$\mathrm{A} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{26} = 0$ & $\mathrm{A}$ regulates $\mathrm{A}$\\
				$\mathrm{B} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{27} = 0$ & $\mathrm{B}$ regulates $\mathrm{B}$\\
				$\mathrm{C} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{28} = 0$ & $\mathrm{C}$ regulates $\mathrm{C}$\\
				$\mathrm{B} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{29} = 0$ & artificial fusion between proteins\\
				$\mathrm{C} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{30} = 0$ & artificial fusion between proteins\\
				$\mathrm{A} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{31} = 0$ & artificial fusion between proteins\\
				$\mathrm{C} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{32} = 0$ & artificial fusion between proteins\\
				$\mathrm{B} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{33} = 0$ & artificial fusion between proteins\\
				$\mathrm{A} + \mathrm{B}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{34} = 0$ & artificial fusion between proteins\\
				$\mathrm{A}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{35} = 0$ & artificial conversion between proteins\\
				$\mathrm{B}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{36} = 0$ & artificial conversion between proteins\\
				$\mathrm{C}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{37} = 0$ & artificial conversion between proteins\\
				$\mathrm{A}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{38} = 0$ & artificial conversion between proteins\\
				$\mathrm{C}$ &$\rightharpoonup$& $\mathrm{B}$ & $k_{39} = 0$ & artificial conversion between proteins\\
				$\mathrm{B}$ &$\rightharpoonup$& $\mathrm{A}$ & $k_{40} = 0$ & artificial conversion between proteins\\
				$\mathrm{mRNA}_\mathrm{B} + \mathrm{mRNA}_\mathrm{C}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{A}$ & $k_{41} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{mRNA}_\mathrm{B}$ &$\rightharpoonup$& $\mathrm{mRNA}_\mathrm{C}$ & $k_{42} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{A}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{43} = 0$ & artificial fusion\\
				$\mathrm{mRNA}_\mathrm{C} + \mathrm{C}$ &$\rightharpoonup$& $\mathrm{C}$ & $k_{43} = 0$ & artificial fusion
			\end{tabular}
		}
		\caption{Full set of ansatz reactions $\Theta$ used in Sec.~\ref{sec:results}.}
		\label{tab:reaction-library}
	\end{table}
\end{document}
